# Multilingual RAG System

A sophisticated **Retrieval-Augmented Generation (RAG) system** that can intelligently answer questions in **Bengali and English** from uploaded PDF documents using state-of-the-art AI technology. This system combines advanced text processing, multilingual embeddings, and powerful language models to provide accurate, context-aware responses.

## ğŸš€ Project Overview

This project implements a complete end-to-end RAG pipeline that can:
- **Extract and process text** from PDF documents (including scanned Bengali documents)
- **Generate multilingual embeddings** for semantic search
- **Answer complex questions** using AI with proper context retrieval
- **Provide a modern web interface** with premium UI/UX design
- **Support cross-lingual queries** with automatic language detection

The system is particularly optimized for Bengali literature and can handle complex literary texts like "Oporichita" by Rabindranath Tagore.

## âš™ï¸ Project Setup

### Prerequisites

Before setting up the project, ensure you have the following installed:

- **Python 3.8+** 
- **Git**
- **Tesseract OCR** (for PDF text extraction)
- **Groq API Key** (for AI-powered answer generation)

### Step 1: Clone the Repository

```bash
git clone https://github.com/adibmahmud007/Bangla-RAG-Agent.git
cd multilingual-rag-system
```

### Step 2: Create Virtual Environment

```bash
# Create virtual environment
python -m venv venv

# Activate virtual environment
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate
```

### Step 3: Install Dependencies

```bash
# Install core dependencies
pip install fastapi uvicorn python-multipart
pip install sentence-transformers scikit-learn numpy pandas
pip install groq python-dotenv
pip install faiss-cpu
pip install pytesseract pdf2image Pillow
pip install pydantic

# For development
pip install pytest black flake8
```

### Step 4: Install Tesseract OCR

#### Windows:
1. Download Tesseract from: https://github.com/UB-Mannheim/tesseract/wiki
2. Install and add to PATH
3. Download Bengali language pack

#### macOS:
```bash
brew install tesseract tesseract-lang
```

#### Ubuntu/Linux:
```bash
sudo apt update
sudo apt install tesseract-ocr tesseract-ocr-ben
sudo apt install poppler-utils
```

### Step 5: Environment Configuration

Create a `.env` file in the root directory:

```env
# Groq API Configuration
GROQ_API_KEY=your_groq_api_key_here

# Tesseract Configuration (Windows only)
TESSERACT_CMD=C:\Program Files\Tesseract-OCR\tesseract.exe

# Optional: Model configurations
EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2
GROQ_MODEL=llama-3.3-70b-versatile
```

### Step 6: Prepare Your Data

1. Place your PDF documents in the `data/` directory
2. The system currently includes `hsc26_bangla1.pdf` as sample data
3. Run the rag model :

```bash
python app/rag_model.py
```

### Step 7: Run the Application

```bash
# Start the FastAPI server
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

The application will be available at:
- **Web Interface**: http://localhost:8000
- **API Documentation**: http://localhost:8000/docs
- **Alternative Docs**: http://localhost:8000/redoc
- **Health Check**: http://localhost:8000/api/status

## ğŸ“¸ Sample Output

### Web Interface
![Premium Web Interface](./Sample_screenshots/1.png)<br>
*Premium glass morphism design with Bengali question input and AI-powered answer display*

### Question Answer Input and Output Interface
![à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦­à¦¾à¦·à¦¾à¦¯à¦¼ à¦¸à§à¦ªà§à¦°à§à¦· à¦•à¦¾à¦•à§‡ à¦¬à¦²à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡?](./Sample_screenshots/2.png)<br>
*Large question input field with Answer field and Related chunks file*

### Described button Interface
![Bengali Describe the context](./Sample_screenshots/3.png)<br>
*When clicked on the describe button users can see the following details about the model, evaluation, matched keywords etc*

## à¦•à§‡ à¦†à¦¸à¦° à¦œà¦®à¦¾à¦‡à¦¤à§‡ à¦…à¦¦à§à¦¬à¦¿à¦¤à§€à§Ÿ?
### à¦‰à¦¤à§à¦¤à¦°à¦ƒ à¦¹à¦°à¦¿à¦¶ 
![Answer Display](./Sample_screenshots/4.png)


## à¦•à¦¾à¦° à¦¬à§Ÿà¦¸ à¦šà¦²à§à¦²à¦¿à¦¶ à¦à¦° à¦à¦ªà¦¾à¦° à¦“à¦ªà¦¾à¦°?
### à¦‰à¦¤à§à¦¤à¦°à¦ƒ à¦¶à¦®à§à¦­à§à¦¨à¦¾à¦¥à¦¬à¦¾à¦¬à§à¦°
![](./Sample_screenshots/5.png)


## à¦­à¦¾à¦—à§à¦¯à¦¦à§‡à¦¬à§à¦¤à¦¾à¦° à¦ªà§à¦°à¦§à¦¾à¦¨ à¦à¦œà§‡à¦¨à§à¦Ÿ à¦•à§‡?
### à¦‰à¦¤à§à¦¤à¦°à¦ƒ à¦®à¦¾à¦®à¦¾
![](./Sample_screenshots/6.png)


### Question-Answer Examples

#### Bengali Query Example:
**à¦ªà§à¦°à¦¶à§à¦¨:** "à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦­à¦¾à¦·à¦¾à¦¯à¦¼ à¦¸à§à¦ªà§à¦°à§à¦· à¦•à¦¾à¦•à§‡ à¦¬à¦²à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡?"
**à¦‰à¦¤à§à¦¤à¦°:** à¦¶à§à¦®à§à¦­à§à¦¨à¦¾à¦¥


## ğŸ“š API Documentation

### Core Endpoints

#### 1. Search and Answer
```http
POST /api/search
Content-Type: application/json

{
  "query": "à¦®à§‡à§Ÿà§‡à¦° à¦¬à§Ÿà¦¸ à¦ªà¦¨à§‡à¦° à¦¶à§à¦¨à§‡ à¦•à¦¾à¦° à¦®à¦¨ à¦­à¦¾à¦° à¦¹à¦²à§‹?"
}
```

**Response:**
```json
{
  "answer": "à¦®à¦¾à¦®à¦¾à¦°",
  "confidence": 0.85,
  "keyword_matches": 4.0,
  "semantic_score":0.766,
  "matching_keywords": ["à¦®à§‡à§Ÿà§‡à¦°", "à¦¬à§Ÿà¦¸", "à¦®à¦¨", "à¦­à¦¾à¦°"],
  "full_chunk": "Complete context used for answer",
  "model": "llama-3.3-70b-versatile",
  "success": true
}
```

#### 2. System Status
```http
GET /api/status
```

**Response:**
```json
{
  "status": "healthy",
  "chunks_loaded": 102,
  "groq_available": true,
  "model": "llama-3.3-70b-versatile"
}
```

## 3. Interactive Documentation
- **Swagger UI**: Available at `http://localhost:8000/docs#/` with interactive testing
### SwaggerUI Samples
#### Swagger Home
![](./Sample_screenshots/swagger_home.png)
#### Question
![](./Sample_screenshots/question.png)
#### Answer
![](./Sample_screenshots/Answer.png)


## ğŸ› ï¸ Technical Implementation
## ğŸ› ï¸ Technology Stack

| Category | Technology | Version | Purpose | Why Used |
|----------|------------|---------|---------|----------|
| **ğŸŒ Backend Framework** | FastAPI | 0.104+ | Web API Framework | Modern, fast, auto-documentation, async support |
| **ğŸš€ ASGI Server** | Uvicorn | 0.24+ | Production Server | High-performance ASGI server for FastAPI |
| **ğŸ§  Language Model** | Groq API | Latest | Answer Generation | Fast, accurate LLM responses (llama-3.3-70b-versatile) |
| **ğŸ”¤ Embedding Model** | SentenceTransformers | 2.2+ | Text Embeddings | Multilingual semantic understanding |
| **ğŸ“Š Vector Database** | FAISS | 1.7+ | Similarity Search | Efficient nearest neighbor search for embeddings |
| **ğŸ‘ï¸ OCR Engine** | Tesseract | 5.0+ | PDF Text Extraction | Bengali OCR support, open-source reliability |
| **ğŸ“„ PDF Processing** | pdf2image | 1.16+ | PDF to Image | Convert PDF pages for OCR processing |
| **ğŸ–¼ï¸ Image Processing** | Pillow (PIL) | 10.0+ | Image Manipulation | Image preprocessing for better OCR results |
| **ğŸ”¢ Scientific Computing** | NumPy | 1.24+ | Numerical Operations | Array operations, mathematical computations |
| **ğŸ“ˆ Data Analysis** | Pandas | 2.0+ | Data Manipulation | DataFrame operations, data cleaning |
| **ğŸ” Text Analysis** | scikit-learn | 1.3+ | TF-IDF, Cosine Similarity | Statistical text analysis and similarity metrics |
| **âš™ï¸ Environment** | python-dotenv | 1.0+ | Environment Variables | Secure API key management |
| **âœ… Data Validation** | Pydantic | 2.4+ | Request/Response Models | Type validation, automatic documentation |
| **ğŸŒ Language** | Python | 3.8+ | Core Programming | Excellent AI/ML ecosystem, readable syntax |

## ğŸ“š Additional Libraries & Tools

| Category | Technology | Purpose |
|----------|------------|---------|
| **ğŸ¨ Frontend** | HTML5, CSS3, JavaScript | Premium web interface with glass morphism |
| **ğŸ”§ Development Tools** | Git, GitHub | Version control and repository hosting |
| **ğŸ“¦ Package Management** | pip, venv | Python package and environment management |
| **ğŸ³ Containerization** | Docker (Future) | Deployment and scalability |
| **â˜ï¸ Cloud Deployment** | AWS/GCP (Future) | Production hosting |

### Architecture Overview

```
MULTILINGUAL_RAG/
â”œâ”€â”€ __pycache__/                 # Python cache files
â”œâ”€â”€ app/                         # Main application directory
â”‚   â”œâ”€â”€ __pycache__/            # App cache files
â”‚   â”œâ”€â”€ chunk_text.py           # Text chunking functionality
â”‚   â”œâ”€â”€ clean_text.py           # Text cleaning utilities
â”‚   â”œâ”€â”€ embed_chunks.py         # Embedding generation
â”‚   â”œâ”€â”€ extract_pdf_text.py     # PDF text extraction
â”‚   â”œâ”€â”€ rag_model.py            # Main RAG model implementation
â”‚   â”œâ”€â”€ search_qa.py            # Search and QA functionality
â”‚   â””â”€â”€ vector_store.pkl        # Pre-trained vector embeddings
â”œâ”€â”€ data/                       # Data directory
â”‚   â””â”€â”€ hsc26_bangla1.pdf      # Sample Bengali PDF document
â”œâ”€â”€ Sample_screenshots/         # UI screenshots for documentation              
â”œâ”€â”€ venv/                       # Virtual environment (ignored in git)
â”œâ”€â”€ .env                        # Environment variables (API keys)
â”œâ”€â”€ .gitignore                  # Git ignore configuration
â”œâ”€â”€ main.py                     # FastAPI server and web interface
â”œâ”€â”€ README.md                   # Project documentation
â””â”€â”€ requirements.txt            # Python dependencies
```


## â“ Technical Q&A

### Text Extraction Method

**Q: What method or library did you use to extract the text, and why? Did you face any formatting challenges with the PDF content?**

**A:** I used **Tesseract OCR with pdf2image** for text extraction. This combination was chosen because:

- **Tesseract OCR**: Excellent support for Bengali text recognition
- **pdf2image**: Converts PDF pages to images for OCR processing
- **Multi-language support**: Handles both Bengali and English text effectively

**Formatting Challenges Faced:**
- Bengali character encoding issues
- Inconsistent spacing in scanned documents
- Mixed language content requiring language detection
- Special characters and punctuation handling

**Solutions Implemented:**
```python
# Text cleaning pipeline
def clean_text(self, text):
    # Remove extra whitespace and normalize
    text = re.sub(r'\s+', ' ', text)
    # Handle Bengali punctuation
    bengali_punctuation = 'à¥¤,;!?()[]{}"\'-â€“â€”'
    text = re.sub(f'[{re.escape(string.punctuation + bengali_punctuation)}]', ' ', text)
    return text.strip()
```

### Chunking Strategy

**Q: What chunking strategy did you choose? Why do you think it works well for semantic retrieval?**

**A:** I implemented a **hybrid chunking strategy** combining:

1. **Sentence-based chunking**: Splits text at sentence boundaries (à¥¤!?)
2. **Character limit chunking**: Maximum 500-1000 characters per chunk
3. **Semantic boundary preservation**: Avoids breaking mid-sentence

**Why this works well:**
- **Maintains context**: Complete sentences preserve semantic meaning
- **Optimal size**: 500-1000 characters balance context and specificity
- **Bengali-aware**: Recognizes Bengali sentence endings (à¥¤)
- **Overlap handling**: Slight overlap between chunks prevents information loss

```python
def intelligent_chunking(self, text, max_chunk_size=800):
    sentences = re.split(r'[à¥¤!?]', text)
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        if len(current_chunk + sentence) < max_chunk_size:
            current_chunk += sentence + "à¥¤"
        else:
            chunks.append(current_chunk.strip())
            current_chunk = sentence + "à¥¤"
    
    return chunks
```

### Embedding Model Choice

**Q: What embedding model did you use? Why did you choose it? How does it capture meaning?**

**A:** I used **`paraphrase-multilingual-MiniLM-L12-v2`** because:

**Why this model:**
- **Multilingual support**: Native support for 50+ languages including Bengali
- **Paraphrase understanding**: Trained to understand semantic similarity
- **Optimal size**: Good balance between performance and computational efficiency
- **Cross-lingual capabilities**: Can find similar content across languages

**How it captures meaning:**
- **Transformer architecture**: Uses attention mechanisms to understand context
- **Semantic embeddings**: Maps text to 384-dimensional vector space
- **Cross-lingual alignment**: Similar concepts in different languages have similar embeddings

```python
# Embedding generation
embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
embeddings = embedding_model.encode(chunks)
```

### Similarity Comparison Method

**Q: How are you comparing the query with stored chunks? Why did you choose this similarity method?**

**A:** I implemented a **multi-layered similarity approach**:

1. **Semantic Similarity** (Cosine similarity on embeddings)
2. **Keyword Matching** (Enhanced with Bengali stemming)
3. **TF-IDF Similarity** (Statistical text analysis)

**Combined scoring formula:**
```python
combined_score = (
    0.60 * keyword_match_ratio +     # Primary: keyword matching
    0.20 * keyword_similarity +      # Secondary: keyword context
    0.10 * tfidf_score +            # Statistical relevance
    0.05 * semantic_score +         # Semantic understanding
    0.05 * length_normalization     # Length bias correction
)
```

**Why this approach:**
- **Keyword matching**: Ensures relevant content is prioritized
- **Semantic similarity**: Captures contextual meaning
- **TF-IDF**: Handles statistical term importance
- **Weighted combination**: Balances different similarity aspects

### Meaningful Query-Document Comparison

**Q: How do you ensure meaningful comparison? What happens with vague queries?**

**A:** **Ensuring Meaningful Comparison:**

1. **Query preprocessing**: Text cleaning and keyword extraction
2. **Language detection**: Automatic Bengali/English detection
3. **Stemming**: Bengali word stemming for better matching
4. **Multi-metric evaluation**: Combined similarity scores

**Handling Vague Queries:**
```python
def handle_vague_query(self, query, chunks):
    if len(query.split()) < 3:  # Very short query
        # Use broader semantic search
        return self.semantic_search_only(query, chunks)
    
    if not self.extract_keywords(query):  # No meaningful keywords
        # Fallback to TF-IDF
        return self.tfidf_search(query, chunks)
```

**Strategies for vague queries:**
- **Broader semantic search**: Rely more on embedding similarity
- **Context expansion**: Use surrounding text for better context
- **Multiple result ranking**: Show confidence scores to user
- **Clarification prompts**: Suggest more specific queries

### Result Relevance and Improvements

**Q: Do the results seem relevant? What might improve them?**

**A:** **Current Relevance Assessment:**

âœ… **Strengths:**
- High accuracy for specific questions (85-90% confidence)
- Good Bengali text understanding
- Effective context retrieval
- Proper handling of literary texts

âš ï¸ **Areas for Improvement:**

1. **Better Chunking:**
```python
# Implement paragraph-aware chunking
def smart_chunking(self, text):
    paragraphs = text.split('\n\n')
    chunks = []
    for para in paragraphs:
        if len(para) > MAX_CHUNK_SIZE:
            # Split long paragraphs at sentence boundaries
            chunks.extend(self.sentence_split(para))
        else:
            chunks.append(para)
    return chunks
```

2. **Enhanced Embedding Models:**
   - Consider `multilingual-e5-large` for better performance
   - Fine-tune embeddings on Bengali literature
   - Use domain-specific embeddings

3. **Larger Document Corpus:**
   - Include more Bengali literature
   - Add cross-references between documents
   - Implement document-level metadata

4. **Advanced Retrieval:**
   - Implement reranking models
   - Use query expansion techniques
   - Add temporal and spatial awareness

**Evaluation Metrics Implemented:**
- **Confidence Score**: 0.0-1.0 based on similarity
- **Keyword Match Count**: Number of query keywords found
- **Semantic Similarity**: Cosine similarity score
- **Answer Quality**: LLM-generated relevance assessment

## ğŸš€ Future Enhancements

- **Multi-document querying**: Search across multiple PDFs simultaneously
- **Advanced OCR**: Better handling of complex layouts and tables
- **Real-time learning**: System improvement based on user feedback
- **API rate limiting**: Production-ready API with authentication
- **Caching system**: Redis integration for faster responses
- **Deployment**: Docker containerization and cloud deployment

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Authos
### Adib Mahmud
- Email: [adibmahmud308@gmail.com]
- Contact: +8801834475079

---

**Built with â¤ï¸ for the Bengali language community and multilingual AI applications**
